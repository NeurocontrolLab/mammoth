#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jun  4 16:26:50 2024

@author: chenyun
"""

import os
from string import Template

code_dict = {}
code_dict['data_recording'] = Template(r'''{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import quantities as pq\n",
    "import joblib\n",
    "import re\n",
    "import shutil\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from dtw import accelerated_dtw\n",
    "from SmartNeo.analysis_layer.tools.dataset_maker.dataset_maker import DatasetMaker\n",
    "from SmartNeo.user_layer.dict_to_neo import templat_neo\n",
    "from SmartNeo.interface_layer.nwb_interface import NWBInterface\n",
    "from SmartNeo.analysis_layer.spike_preprocessor3 import SpikeStatistics\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from elephant.kernels import GaussianKernel\n",
    "from probeinterface import read_probeinterface,Probe, ProbeGroup\n",
    "from probeinterface.plotting import plot_probe\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% read data\n",
    "# data folder\n",
    "raw_dirname = '${session0}'\n",
    "formated_data_path = os.path.join(raw_dirname, 'formatted_data')\n",
    "\n",
    "# read formated SmartNeo data\n",
    "nwb_saver = NWBInterface()\n",
    "neural_data = nwb_saver.read_nwb(filename = os.path.join(formated_data_path,'neural_data.nwb'))\n",
    "\n",
    "nwb_saver = NWBInterface()\n",
    "trial_behavior = nwb_saver.read_nwb(filename = os.path.join(formated_data_path,'trial_behavior.nwb'))\n",
    "\n",
    "continuous_saver = NWBInterface()\n",
    "continuous_behavior = nwb_saver.read_nwb(filename = os.path.join(formated_data_path,'continuous_bhv_block.nwb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "description_path = os.path.join(raw_dirname, 'description')\n",
    "cmp_name = [i for i in os.listdir(description_path) if 'cmp' in i][0]\n",
    "\n",
    "with open(os.path.join(description_path, cmp_name),'r') as f:\n",
    "    probe_info = f.readlines()[14::]\n",
    "\n",
    "probe_info = [i.split() for i in probe_info][0:-1]\n",
    "array_name = ['elec'] if '-' not in probe_info[0][-1]\\\n",
    "    else list(set([i[-1].split('-')[0] for i in probe_info]))\n",
    "\n",
    "nchannels_per_array = len(probe_info)\n",
    "electrode_counter = []\n",
    "probegroup = ProbeGroup()\n",
    "\n",
    "for array in array_name:\n",
    "    # create an electrode group for this shank\n",
    "    \n",
    "    probe_2d = Probe(ndim=2, si_units='um')\n",
    "    probe_2d.annotate(\n",
    "        name = array, \n",
    "        manufacturer=\"blackrock microsystem\",\n",
    "        escription = 'one 96 Utah array'\n",
    "        )\n",
    "    positions = []\n",
    "    device_channel = []\n",
    "    # test2 = []\n",
    "    # add electrodes to the electrode table\n",
    "    for ielec in probe_info:\n",
    "        if array not in ielec[-1]:\n",
    "            continue\n",
    "    \n",
    "        positions.append([float(ielec[0])*400, float(ielec[1])*400])\n",
    "        device_channel.append((ord(ielec[2])-65)*32+int(ielec[3])-1)\n",
    "        # test2.append(ielec[2]+ielec[3])\n",
    "        \n",
    "    \n",
    "    probe_2d.set_contacts(positions=np.array(positions), \n",
    "                          shapes='circle', \n",
    "                          shape_params={'radius': 20})\n",
    "    probe_2d.set_device_channel_indices(device_channel)\n",
    "    probe_2d.create_auto_shape(probe_type='tip')\n",
    "    probegroup.add_probe(probe_2d)\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(2,1)\n",
    "plot_probe(probegroup.probes[0], ax=ax1)\n",
    "plot_probe(probegroup.probes[1], ax=ax2)\n",
    "ax2.title.set_text('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [i.index for i in trial_behavior.segments]\n",
    "sorted_index = np.argsort(index)\n",
    "trials = [trial_behavior.segments[i] for i in sorted_index]\n",
    "\n",
    "# index\n",
    "trial_index = [i.index for i in trials]\n",
    "# event\n",
    "event = {}\n",
    "event['Labels'] = [i.events[0].labels for i in trials]\n",
    "event['Times'] = [i.events[0].times for i in trials]\n",
    "df_event = pd.DataFrame(event)\n",
    "df_event.columns=[len(df_event.columns)*['Event'], list(df_event.columns)]\n",
    "df_event.index=trial_index\n",
    "\n",
    "# description\n",
    "description = {}\n",
    "description['Block'] = [i.description['Block'] for i in trials]\n",
    "description['AngularV'] = [float(i.description['UserVars']['angularV'].squeeze()) for i in trials]\n",
    "description['AbsoluteTrialStartTime'] = [i.description['AbsoluteTrialStartTime'] for i in trials]\n",
    "description['Condition'] = [i.description['Condition'] for i in trials]\n",
    "description['TrialError'] = [i.description['TrialError'] for i in trials]\n",
    "df_description = pd.DataFrame(description)\n",
    "df_description.columns=[len(df_description.columns)*['Description'], list(df_description.columns)]\n",
    "df_description.index=trial_index\n",
    "\n",
    "\n",
    "# ObjectStatusRecord\n",
    "ObjectStatusRecord = {}\n",
    "obj_pos = [[j for j in i.irregularlysampledsignals if 'Position' in j.name][0] for i in trials]\n",
    "obj_sta = [[j for j in i.irregularlysampledsignals if 'Status' in j.name][0] for i in trials]\n",
    "ObjectStatusRecord['Position'] = [np.array(i) for i in obj_pos]\n",
    "ObjectStatusRecord['Status'] = [np.array(i) for i in obj_sta]\n",
    "df_ObjectStatusRecord = pd.DataFrame(ObjectStatusRecord)\n",
    "df_ObjectStatusRecord.columns=[len(df_ObjectStatusRecord.columns)*['ObjectStatusRecord'], list(df_ObjectStatusRecord.columns)]\n",
    "df_ObjectStatusRecord.index=trial_index\n",
    "\n",
    "frames = [df_event, df_description, df_ObjectStatusRecord]\n",
    "df_trials = pd.concat(frames,axis=1).rename_axis('TrialIndex')\n",
    "\n",
    "df_trials.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time consistency check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_event = [i for i in neural_data.segments if i.name=='RecordingSystemEvent'][0]\n",
    "neural_event_labels = neural_event.events[0].labels\n",
    "neural_event_times = neural_event.events[0].times\n",
    "\n",
    "ml_event_labels = np.concatenate(df_trials['Event']['Labels'].to_list())\n",
    "ml_t = df_trials['Event']['Times'].to_list()\n",
    "ml_abs_t = df_trials['Description']['AbsoluteTrialStartTime'].to_list()\n",
    "ml_event_times = np.concatenate([(i+j).rescale(pq.s) for i,j in zip(ml_t, ml_abs_t)])*pq.s\n",
    "\n",
    "li_distance = lambda x, y: 0 if np.abs(x - y)==0 else len(neural_event_labels)\n",
    "    # manhattan_distance = lambda x, y: np.abs(x - y)\n",
    "\n",
    "_,_,_,path1 = accelerated_dtw(ml_event_labels, neural_event_labels, dist=li_distance)\n",
    "path1 = np.array(path1).T\n",
    "zero_dir = (ml_event_labels[path1[:,0]]-neural_event_labels[path1[:,1]])==0\n",
    "assert len(neural_event_labels)-sum(zero_dir)<(0.005*len(neural_event_labels)), 'wrong alignment'\n",
    "\n",
    "diff_time = ml_event_times[path1[:,0]]-neural_event_times[path1[:,1]]\n",
    "diff_time = diff_time[np.where(np.abs(diff_time-np.mean(diff_time))<0.05)[0]]\n",
    "plt.boxplot(diff_time-np.mean(diff_time))\n",
    "print(len(diff_time)/len(neural_event_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_time_mean = np.mean(diff_time)\n",
    "st = [i for i in neural_data.segments if i.name=='kilosort2.5'][0].spiketrains\n",
    "st_shift = [i.time_shift(diff_time_mean) for i in st]\n",
    "\n",
    "kwargs_list = []\n",
    "trial_ind = []\n",
    "move_direction = []\n",
    "speed = []\n",
    "\n",
    "for ind, i in enumerate(df_trials.iloc):\n",
    "    Description = i['Description']\n",
    "    Event = i['Event']\n",
    "    ObjectStatusRecord = i['ObjectStatusRecord']\n",
    "    if 0!= Description['TrialError']:\n",
    "        continue\n",
    "    \n",
    "    kwargs = {'t_start': Event['Times'][Event['Labels']==5]-1*pq.s+Description['AbsoluteTrialStartTime'],\n",
    "              't_stop': Event['Times'][Event['Labels']==5]+1*pq.s+Description['AbsoluteTrialStartTime'],\n",
    "              'aligned_marker':[5],\n",
    "              'aligned_marker2':[5],\n",
    "              'trial_index': ind}\n",
    "    kwargs_list.append(kwargs)\n",
    "    trial_ind.append(ind)\n",
    "    move_direction.append(ObjectStatusRecord['Position'][3][4])\n",
    "    speed.append(Description['AngularV'])\n",
    "\n",
    "trial_ind = np.array(trial_ind)\n",
    "    \n",
    "kwargs = {\n",
    "    'kernel' : GaussianKernel(50*pq.ms),\n",
    "    'sampling_period' : 50*pq.ms\n",
    "}\n",
    "\n",
    "sliced_st = SpikeStatistics.preprocessing('spike_time', kwargs_list, st_shift)\n",
    "sliced_is = SpikeStatistics.preprocessing('instantaneous_rate', kwargs_list, st_shift, **kwargs)\n",
    "sliced_is = sliced_is[trial_ind,:,5:-5]\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model\n",
    "\n",
    "move_direction = np.array(move_direction)\n",
    "# time independent\n",
    "import scipy\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "time_suc = []\n",
    "for t_i in range(sliced_is.shape[-1]):\n",
    "    X = sliced_is[:,:,t_i].reshape((sliced_is.shape[0],-1))\n",
    "    Y = move_direction\n",
    "    reg = linear_model.MultiTaskLasso()\n",
    "    scores = np.mean(cross_val_score(reg, X, Y, cv=5, scoring='r2',n_jobs=-1))\n",
    "    time_suc.append(scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_st_shift = [i.time_shift(diff_time_mean+2*pq.s) for i in st]\n",
    "shuffled_sliced_is = SpikeStatistics.preprocessing('instantaneous_rate', kwargs_list, shuffled_st_shift, **kwargs)\n",
    "shuffled_sliced_is = shuffled_sliced_is[trial_ind,:,5:-5]\n",
    "\n",
    "shuffled_time_suc = []\n",
    "for t_i in range(sliced_is.shape[-1]):\n",
    "    X = shuffled_sliced_is[:,:,t_i].reshape((sliced_is.shape[0],-1))\n",
    "    Y = move_direction\n",
    "    reg = linear_model.MultiTaskLasso()\n",
    "    scores = np.mean(cross_val_score(reg, X, Y, cv=5, scoring='r2',n_jobs=-1))\n",
    "    shuffled_time_suc.append(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "data = {}\n",
    "# data[\"Times\"] = np.arange(-2,2,0.05)[5:-5]\n",
    "data[\"Times\"] = list(np.arange(0,30,1))+list(np.arange(0,30,1))\n",
    "data['R^2'] = time_suc + shuffled_time_suc\n",
    "data['Mismatch'] = ['Aligned']*len(time_suc)+['2 sec lag']*len(shuffled_time_suc)\n",
    "sns.lineplot(x=\"Times\", y='R^2',data=data, hue='Mismatch', ax=ax1)\n",
    "sta, mid, end = data[\"Times\"][4], data[\"Times\"][14], data[\"Times\"][24]\n",
    "# ax1.set_xticks([-0.5,0,0.5,1,1.5],['-1','-0.5','0','0.5','1'],fontsize=15,rotation=45)\n",
    "ax1.set_xticks([4,14,24],['-0.5','0','0.5'],fontsize=15,rotation=45)\n",
    "ax1.set_yticks([0,0.5,1],[0,0.5,1],fontsize=15)\n",
    "ax1.set_ylabel('R-square',fontsize=15)\n",
    "ax1.set_xlabel('Times (s)',fontsize=15)\n",
    "ax1.set_xlim([0,29])\n",
    "ax1.set_ylim([-0.2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = move_direction\n",
    "sliced_is.shape\n",
    "move_direction.shape\n",
    "\n",
    "angles = np.arctan2(move_direction[:,1], move_direction[:,0]) * 180 / np.pi\n",
    "groups = np.digitize(angles % 360, bins=np.linspace(0, 360, num=9)) * 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "folder_name = os.path.join(description_path, 'psth')\n",
    "\n",
    "if os.path.exists(folder_name):\n",
    "    shutil.rmtree(folder_name)\n",
    "\n",
    "os.mkdir(folder_name)\n",
    "\n",
    "plt.ioff()\n",
    "for i in tqdm(range(sliced_is.shape[1])):\n",
    "    neural_matrix = sliced_is[:,i,:]  \n",
    "    n_trials = sliced_is.shape[0]     \n",
    "    n_timepoints = sliced_is.shape[-1] \n",
    "\n",
    "    time_points = np.tile(np.arange(n_timepoints), (n_trials,1)).flatten()\n",
    "    directions = np.tile(groups, (n_timepoints,1)).T.flatten()\n",
    "    firing_rates = neural_matrix.flatten()\n",
    "    directions = ['{} degree'.format(i) for i in directions]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Time': time_points,\n",
    "        'Direction': directions,\n",
    "        'Firing Rate': firing_rates,\n",
    "    })\n",
    "\n",
    "    sns.set(style=\"white\", palette=\"muted\")\n",
    "    plt.figure(figsize=(10,6))\n",
    "    psth_plot = sns.lineplot(x='Time', y='Firing Rate', hue='Direction', data=df, ci=95, estimator=\"mean\")\n",
    "\n",
    "    plt.title('Grouped Single Cell PSTH with SEM', fontsize=16)\n",
    "    plt.xlabel('Time (50 ms)', fontsize=14)\n",
    "    plt.ylabel('Firing Rate (spikes/ms)', fontsize=14)\n",
    "\n",
    "    sns.despine()\n",
    "\n",
    "    plt.xlim(df['Time'].min(), df['Time'].max())\n",
    "    plt.ylim(0, df['Firing Rate'].max() + df['Firing Rate'].std())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = f'{folder_name}/index_{i}.png'\n",
    "    plt.savefig(save_path)\n",
    "    \n",
    "plt.ion()\n",
    "    \n",
    "i = 0\n",
    "neural_matrix = sliced_is[:,i,:]  \n",
    "n_trials = sliced_is.shape[0]     \n",
    "n_timepoints = sliced_is.shape[-1] \n",
    "\n",
    "time_points = np.tile(np.arange(n_timepoints), (n_trials,1)).flatten()\n",
    "directions = np.tile(groups, (n_timepoints,1)).T.flatten()\n",
    "firing_rates = neural_matrix.flatten()\n",
    "directions = ['{} degree'.format(i) for i in directions]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Time': time_points,\n",
    "    'Direction': directions,\n",
    "    'Firing Rate': firing_rates,\n",
    "})\n",
    "\n",
    "sns.set(style=\"white\", palette=\"muted\")\n",
    "plt.figure(figsize=(10,6))\n",
    "psth_plot = sns.lineplot(x='Time', y='Firing Rate', hue='Direction', data=df, ci=95, estimator=\"mean\")\n",
    "\n",
    "plt.title('Grouped Single Cell PSTH with SEM', fontsize=16)\n",
    "plt.xlabel('Time (ms)', fontsize=14)\n",
    "plt.ylabel('Firing Rate (spikes/ms)', fontsize=14)\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "plt.xlim(df['Time'].min(), df['Time'].max())\n",
    "plt.ylim(0, df['Firing Rate'].max() + df['Firing Rate'].std())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vicon_traj = continuous_behavior.segments[0].irregularlysampledsignals[0]\n",
    "time_diff = ml_event_times[np.where(ml_event_labels==24)[0]]-vicon_traj.times[0]\n",
    "vt_shift = vicon_traj.time_shift(time_diff+(800)*pq.ms)\n",
    "des = [i.description for i in st_shift]\n",
    "ele = np.array([i['electrode'] for i in des])\n",
    "\n",
    "kwargs_list = []\n",
    "trial_ind = []\n",
    "move_direction = []\n",
    "speed = []\n",
    "move_traj = []\n",
    "\n",
    "for ind, i in enumerate(df_trials.iloc):\n",
    "    Description = i['Description']\n",
    "    Event = i['Event']\n",
    "    ObjectStatusRecord = i['ObjectStatusRecord']\n",
    "    if 0!= Description['TrialError']:\n",
    "        continue\n",
    "    \n",
    "    kwargs = {'t_start': Event['Times'][Event['Labels']==5]-1*pq.s+Description['AbsoluteTrialStartTime'],\n",
    "              't_stop': Event['Times'][Event['Labels']==5]+1*pq.s+Description['AbsoluteTrialStartTime'],\n",
    "              'aligned_marker':[5],\n",
    "              'aligned_marker2':[5],\n",
    "              'trial_index': ind}\n",
    "    kwargs_list.append(kwargs)\n",
    "    trial_ind.append(ind)\n",
    "    move_direction.append(ObjectStatusRecord['Position'][3][4])\n",
    "    speed.append(Description['AngularV'])\n",
    "    move_traj.append(vt_shift.time_slice(kwargs['t_start'],kwargs['t_stop']))\n",
    "    \n",
    "trial_ind = np.array(trial_ind)\n",
    "    \n",
    "kwargs = {\n",
    "    'kernel' : GaussianKernel(50*pq.ms),\n",
    "    'sampling_period' : 25*pq.ms\n",
    "}\n",
    "\n",
    "sliced_st = SpikeStatistics.preprocessing('spike_time', kwargs_list, st_shift)\n",
    "sliced_is = SpikeStatistics.preprocessing('instantaneous_rate', kwargs_list, st_shift, **kwargs)\n",
    "sliced_is = sliced_is[trial_ind,:,:]\n",
    "\n",
    "resample_mt = np.array([i.resample(int(sliced_is.shape[-1])+1) for i in move_traj])[:,1::,:]\n",
    "\n",
    "Y = resample_mt[:,1::,[0,2]][:,25:48,:]\n",
    "Y_train = Y[0:int(len(Y)/2)].reshape((-1,Y.shape[-1]))\n",
    "Y_predict = Y[int(len(Y)/2)::].reshape((-1,Y.shape[-1]))\n",
    "\n",
    "X = sliced_is.swapaxes(1,-1)[:,25:48,:]\n",
    "X_train = X[0:int(len(Y)/2)].reshape((-1,X.shape[-1]))\n",
    "X_predict = X[int(len(Y)/2)::].reshape((-1,X.shape[-1]))\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_hat = regressor.predict(X_predict)\n",
    " \n",
    "r_squared = r2_score(Y_pred_hat, Y_predict)\n",
    "\n",
    "Y_pred_hat = Y_pred_hat.reshape((-1,*Y.shape[1::]))\n",
    "Y_predict = Y_predict.reshape((-1,*Y.shape[1::]))\n",
    "X_predict = X_predict.reshape((-1,*X.shape[1::]))\n",
    "        \n",
    "move_direction_ = np.array(move_direction)[int(len(Y)/2)::]\n",
    "angles = np.arctan2(move_direction_[:,1], move_direction_[:,0]) * 180 / np.pi\n",
    "groups = np.digitize(angles % 360, bins=np.linspace(0, 360, num=9)) * 45\n",
    "        \n",
    "unique_values, indices = np.unique(groups, return_inverse=True)\n",
    "counts = np.bincount(indices)\n",
    "\n",
    "group_min = min(counts)\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "plot_input = {}\n",
    "for i in sorted(np.unique(groups)):\n",
    "    trial = np.where(groups==i)[0][0:group_min]        \n",
    "    plot_input[i] = {}\n",
    "    plot_input[i]['Y_predict'] = Y_predict[trial]\n",
    "    plot_input[i]['X_predict'] = X_predict[trial]\n",
    "    plot_input[i]['Y_pred_hat'] = Y_pred_hat[trial]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "fig = plt.figure(dpi=300,\n",
    "                 constrained_layout=True,\n",
    "                 figsize=(10,5)\n",
    "                )\n",
    "\n",
    "\n",
    "gs = GridSpec(5, 10, figure=fig, width_ratios=[1]+[15]*8+[1])\n",
    "\n",
    "\n",
    "all_traj = np.array([plot_input[i]['Y_predict'][:,:,0] for i in plot_input])\n",
    "all_traj_y = np.array([plot_input[i]['Y_predict'][:,:,1] for i in plot_input])\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "cmap = mcolors.LinearSegmentedColormap.from_list(\"n\",['black', 'darkred', 'red', 'orange', 'yellow', 'white'])\n",
    "\n",
    "for ind, i in enumerate(plot_input):\n",
    "    arr = plot_input[i]['X_predict']\n",
    "    a_m = arr.mean(0).T\n",
    "    d1 = scaler.fit_transform(a_m[ele==0].T).T\n",
    "    d2 = scaler.fit_transform(a_m[ele==1].T).T\n",
    "    \n",
    "    if ind==0:\n",
    "        max_value_location = np.argmax(d1, axis=1)\n",
    "        d1_sort = np.argsort(max_value_location)\n",
    "        \n",
    "        max_value_location = np.argmax(d2, axis=1)\n",
    "        d2_sort = np.argsort(max_value_location)\n",
    "    d1 = d1[d1_sort]\n",
    "    d2 = d2[d2_sort]\n",
    "    ax1 = fig.add_subplot(gs[0, ind+1])\n",
    "    if ind==0:\n",
    "        sns.heatmap(d1, ax=ax1, cmap=cmap,cbar=False)\n",
    "        ax11 = fig.add_subplot(gs[0, 9])\n",
    "        norm = plt.Normalize(d1.min(), d1.max())\n",
    "        \n",
    "        sm = plt.cm.ScalarMappable(norm=norm)\n",
    "        sm.set_cmap(cmap)\n",
    "        sm.set_array([])\n",
    "        cbar = fig.colorbar(sm, cax=ax11)\n",
    "        cbar.set_ticks([0, 0.5, 1])\n",
    "        cbar.set_ticklabels(['0', '0.5', '1'])\n",
    "        \n",
    "    else:\n",
    "        sns.heatmap(d1, ax=ax1,cbar=False,cmap=cmap)\n",
    "    ax1.axvline(x=10, color='black', linestyle='--', linewidth=1)\n",
    "    \n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['left'].set_visible(False)\n",
    "    ax1.spines['bottom'].set_visible(False)\n",
    "    ax1.get_yaxis().set_ticks_position('none')\n",
    "    ax1.get_xaxis().set_ticks_position('none')\n",
    "    ax1.set_xlabel('')\n",
    "    ax1.set_ylabel('')\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_xticks([])\n",
    "    if ind==0:\n",
    "        ax1.set_ylabel('PMd')\n",
    "    \n",
    "    ax1 = fig.add_subplot(gs[1:3, ind+1])\n",
    "    \n",
    "    if ind==0:\n",
    "        sns.heatmap(d1, ax=ax1, cmap=cmap,cbar=False)\n",
    "        ax11 = fig.add_subplot(gs[1:3, 9])\n",
    "        norm = plt.Normalize(d1.min(), d1.max())\n",
    "        sm = plt.cm.ScalarMappable(norm=norm)\n",
    "        sm.set_cmap(cmap)\n",
    "        sm.set_array([])\n",
    "        cbar = fig.colorbar(sm, cax=ax11)\n",
    "        cbar.set_ticks([0, 0.5, 1])\n",
    "        cbar.set_ticklabels(['0', '0.5', '1'])\n",
    "        \n",
    "    else:\n",
    "        sns.heatmap(d2, ax=ax1,cbar=False,cmap=cmap)\n",
    "    \n",
    "    ax1.axvline(x=10, color='black', linestyle='--', linewidth=1)\n",
    "    \n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['left'].set_visible(False)\n",
    "    ax1.spines['bottom'].set_visible(False)\n",
    "    ax1.get_yaxis().set_ticks_position('none')\n",
    "    ax1.get_xaxis().set_ticks_position('none')\n",
    "    ax1.set_xlabel('')\n",
    "    ax1.set_ylabel('')\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_xticks([])\n",
    "    if ind==0:\n",
    "        ax1.set_ylabel('M1')\n",
    "\n",
    "for ind, i in enumerate(plot_input):\n",
    "    arr = plot_input[i]['Y_predict']\n",
    "    arr1 = plot_input[i]['Y_pred_hat']\n",
    "    ax1 = fig.add_subplot(gs[4, ind+1])\n",
    "    for j in arr:\n",
    "        ax1.plot(j[:,0],j[:,1], color='black', alpha=0.1, linewidth=1.5)\n",
    "    for j in arr1:\n",
    "        ax1.plot(j[:,0],j[:,1], color='red', alpha=0.1, linewidth=1.5)\n",
    "    ax1.set_xlim(np.min(all_traj)-20, np.max(all_traj)+20)\n",
    "    ax1.set_ylim(np.min(all_traj_y)-20, np.max(all_traj_y)+20)\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['left'].set_visible(False)\n",
    "    ax1.spines['bottom'].set_visible(False)\n",
    "    ax1.get_yaxis().set_ticks_position('none')\n",
    "    ax1.get_xaxis().set_ticks_position('none')\n",
    "    ax1.set_xlabel('')\n",
    "    ax1.set_ylabel('')\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_xticks([])\n",
    "    \n",
    "    \n",
    "for ind, i in enumerate(plot_input):\n",
    "    arr = plot_input[i]['Y_predict'][:,:,0]\n",
    "    arr1 = plot_input[i]['Y_pred_hat'][:,:,0]\n",
    "    x = np.array([range(arr.shape[1])]*arr.shape[0])\n",
    "    #index = index + arr.shape[1]\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'x': x.flatten(),\n",
    "        'y': arr.flatten(),\n",
    "        'y1': arr1.flatten()\n",
    "    })\n",
    "    \n",
    "    ax1 = fig.add_subplot(gs[3, ind+1])\n",
    "    if ind != 7:\n",
    "        line1 = sns.lineplot(data=df, x='x', y='y', errorbar=('ci', 95), color='black',ax=ax1)\n",
    "        line2 = sns.lineplot(data=df, x='x', y='y1', errorbar=('ci', 95), color='red',ax=ax1)\n",
    "    else:\n",
    "        df = pd.DataFrame({\n",
    "            'x': list(x.flatten())+list(x.flatten()),\n",
    "            'y': list(arr.flatten())+list(arr1.flatten()),\n",
    "            'group': ['Hand']*len(arr.flatten())+['Decoder']*len(arr1.flatten())\n",
    "        })\n",
    "        sns.lineplot(data=df, x='x', y='y', hue='group', errorbar=('ci', 95), palette={'Decoder': 'red', 'Hand': 'black'})\n",
    "        \n",
    "        ax1.legend(loc=0,frameon=False, prop = {'size':5})\n",
    "        ax1.set_xticks([10, 18], ['0', '0.2'])\n",
    "        \n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    # ax1.spines['left'].set_visible(False)\n",
    "    # ax1.get_yaxis().set_ticks_position('none')\n",
    "    ax1.set_ylim(np.min(all_traj)-20, np.max(all_traj)+20)\n",
    "    ax1.set_xlim(0, X.shape[1])\n",
    "    ax1.set_ylabel('')\n",
    "    if ind != 0:\n",
    "        ax1.set_yticks([])\n",
    "    else:\n",
    "        ax1.set_yticks([-200, 200],[-20, 20])\n",
    "        ax1.set_ylabel('X pos. (cm)')\n",
    "        \n",
    "    ax1.set_xticks([10, 18], ['0', '0.2'])\n",
    "    ax1.set_xlabel('Time (s)')\n",
    "    ax1.axvline(x=10, color='black', linestyle='--', linewidth=1)\n",
    "    ax1.xaxis.set_ticks_position('bottom')  \n",
    "\n",
    "# fig_path = os.path.join(raw_dirname,'description')\n",
    "# plt.savefig(f'{fig_path}/fig_cat.png', dpi=300)\n",
    "print(f'R\\u00B2 is {r_squared}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TCR = [i for i in neural_data.segments if i.name=='TCR'][0]\n",
    "kilo = [i for i in neural_data.segments if i.name=='kilosort2.5'][0]\n",
    "data_list = []\n",
    "for i in TCR.spiketrains:\n",
    "    for j in kilo.spiketrains:\n",
    "        data = {}\n",
    "        if i.description['chn']==(j.description['chn']):\n",
    "            tcr_time = (i.times.rescale(pq.s) -i.t_start.rescale(pq.s)).magnitude*1000\n",
    "            kilo_time = (j.times.rescale(pq.s) -j.t_start.rescale(pq.s)).magnitude*1000\n",
    "            rescale_time = lambda x: np.where(np.histogram(x,range=[0,np.floor(x[-1])],bins = int(np.floor(x[-1])/1))[0]!=0)[0]\n",
    "            chn_per = len(set(rescale_time(tcr_time)) & set(rescale_time(kilo_time)))/len(j)\n",
    "            data['chn'] = 'unshuffled'\n",
    "            data['chn_per'] = chn_per\n",
    "            data_list.append(data)\n",
    "\n",
    "\n",
    "# df = pd.DataFrame(data_list)\n",
    "# df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_num = len(TCR.spiketrains)\n",
    "for j,ind in zip(kilo.spiketrains, np.random.choice(range(ch_num),ch_num)):\n",
    "    i  = TCR.spiketrains[ind]\n",
    "    data = {}\n",
    "    tcr_time = (i.times.rescale(pq.s) -i.t_start.rescale(pq.s)).magnitude*1000\n",
    "    kilo_time = (j.times.rescale(pq.s) -j.t_start.rescale(pq.s)).magnitude*1000\n",
    "    rescale_time = lambda x: np.where(np.histogram(x,range=[0,np.floor(x[-1])],bins = int(np.floor(x[-1])/1))[0]!=0)[0]  \n",
    "    chn_per = len(set(rescale_time(tcr_time)) & set(rescale_time(kilo_time)))/len(j)\n",
    "    data['chn'] = 'shuffled'\n",
    "    data['chn_per'] = chn_per\n",
    "    data_list.append(data)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.barplot(x=\"chn\",y=\"chn_per\",data=df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
''')

code_dict['brain_control'] = Template(r'''{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import quantities as pq\n",
    "import joblib\n",
    "import re\n",
    "import shutil\n",
    "import seaborn as sns\n",
    "from dtw import accelerated_dtw\n",
    "from SmartNeo.analysis_layer.tools.dataset_maker.dataset_maker import DatasetMaker\n",
    "from SmartNeo.user_layer.dict_to_neo import templat_neo\n",
    "from SmartNeo.interface_layer.nwb_interface import NWBInterface\n",
    "from SmartNeo.analysis_layer.spike_preprocessor3 import SpikeStatistics\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from probeinterface import read_probeinterface,Probe, ProbeGroup\n",
    "from probeinterface.plotting import plot_probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dirname = '${session0}'\n",
    "\n",
    "nwb_saver = NWBInterface()\n",
    "bhv_data = nwb_saver.read_nwb(filename = os.path.join(raw_dirname,'formatted_data', 'continuous_behavior.nwb'))\n",
    "\n",
    "nwb_saver = NWBInterface()\n",
    "neural_data = nwb_saver.read_nwb(filename = os.path.join(raw_dirname,'formatted_data', 'neural_data_no_sort.nwb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_path = os.path.join(raw_dirname, 'description')\n",
    "cmp_name = [i for i in os.listdir(description_path) if 'cmp' in i][0]\n",
    "\n",
    "with open(os.path.join(raw_dirname, 'description', cmp_name),'r') as f:\n",
    "    probe_info = f.readlines()[14::]\n",
    "\n",
    "probe_info = [i.split() for i in probe_info][0:-1]\n",
    "array_name = ['elec'] if '-' not in probe_info[0][-1]\\\n",
    "    else list(set([i[-1].split('-')[0] for i in probe_info]))\n",
    "\n",
    "nchannels_per_array = len(probe_info)\n",
    "electrode_counter = []\n",
    "probegroup = ProbeGroup()\n",
    "\n",
    "for array in array_name:\n",
    "    # create an electrode group for this shank\n",
    "    \n",
    "    probe_2d = Probe(ndim=2, si_units='um')\n",
    "    probe_2d.annotate(\n",
    "        name = array, \n",
    "        manufacturer=\"blackrock microsystem\",\n",
    "        escription = 'one 96 Utah array'\n",
    "        )\n",
    "    positions = []\n",
    "    device_channel = []\n",
    "    # test2 = []\n",
    "    # add electrodes to the electrode table\n",
    "    for ielec in probe_info:\n",
    "        if array not in ielec[-1]:\n",
    "            continue\n",
    "    \n",
    "        positions.append([float(ielec[0])*400, float(ielec[1])*400])\n",
    "        device_channel.append((ord(ielec[2])-65)*32+int(ielec[3])-1)\n",
    "        # test2.append(ielec[2]+ielec[3])\n",
    "        \n",
    "    \n",
    "    probe_2d.set_contacts(positions=np.array(positions), \n",
    "                          shapes='circle', \n",
    "                          shape_params={'radius': 20})\n",
    "    probe_2d.set_device_channel_indices(device_channel)\n",
    "    probe_2d.create_auto_shape(probe_type='tip')\n",
    "    probegroup.add_probe(probe_2d)\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(2,1)\n",
    "plot_probe(probegroup.probes[0], ax=ax1)\n",
    "plot_probe(probegroup.probes[1], ax=ax2)\n",
    "ax2.title.set_text('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_pd = {}\n",
    "for i in bhv_data.segments[1].irregularlysampledsignals:\n",
    "    coeff_pd[i.name[1]] = i.as_array().squeeze()\n",
    "coeff_pd = pd.DataFrame(coeff_pd)\n",
    "# coeff_pd.rename_axis('Times')\n",
    "coeff_pd.index = bhv_data.segments[1].irregularlysampledsignals[0].times\n",
    "coeff_pd.index.name = 'Times'\n",
    "coeff_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pd = {}\n",
    "for i in bhv_data.segments[3].irregularlysampledsignals:\n",
    "    pos_pd[i.name[1]] = list(i.as_array().squeeze())\n",
    "\n",
    "pos_pd = pd.DataFrame(pos_pd)\n",
    "pos_pd.rename_axis('Times')\n",
    "pos_pd.index = bhv_data.segments[3].irregularlysampledsignals[0].times\n",
    "pos_pd.index.name = 'Times'\n",
    "pos_unit = bhv_data.segments[3].irregularlysampledsignals[0].times.units\n",
    "\n",
    "pos_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_pd = {}\n",
    "for i in bhv_data.segments[-1].irregularlysampledsignals:\n",
    "    trial_pd[i.name[1]] = list(i.as_array().squeeze())\n",
    "\n",
    "trial_pd = pd.DataFrame(trial_pd)\n",
    "trial_pd.rename_axis('Times')\n",
    "trial_pd.index = bhv_data.segments[-1].irregularlysampledsignals[0].times\n",
    "trial_pd.index.name = 'Times'\n",
    "trial_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = bhv_data.segments[0].events[0]\n",
    "marker_24_pos = np.where(events.labels==24)[0]\n",
    "marker_5_pos = np.where(events.labels==5)[0]\n",
    "\n",
    "trial_event = [events[i:j+1] for i,j in zip(marker_24_pos,marker_5_pos)]\n",
    "\n",
    "marker_24_time = events.times[np.where(events.labels==24)[0]]\n",
    "marker_5_time = events.times[np.where(events.labels==5)[0]]\n",
    "trial_pos = [pos_pd.iloc[np.where(pos_pd.index<j * (pos_pd.index>i))[0]] for i,j in zip(marker_24_time,marker_5_time)]\n",
    "trial_coeff = [coeff_pd.iloc[np.where(coeff_pd.index<j * (coeff_pd.index>i))[0]] for i,j in zip(marker_24_time,marker_5_time)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time consistency check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_event = [i for i in neural_data.segments if i.name=='RecordingSystemEvent'][0]\n",
    "neural_event_labels = neural_event.events[0].labels\n",
    "neural_event_times = neural_event.events[0].times\n",
    "neural_event_labels[neural_event_labels==18]=20\n",
    "\n",
    "ml_event_labels = bhv_data.segments[0].events[0].labels\n",
    "ml_event_times = bhv_data.segments[0].events[0].times\n",
    "\n",
    "d_f = [ml_event_labels[i:i+len(neural_event_labels)]-neural_event_labels for i in range(len(ml_event_labels)-len(neural_event_labels))]\n",
    "start_ind = np.argmin([sum(np.abs(i)) for i in d_f])\n",
    "\n",
    "ml_event_labels = bhv_data.segments[0].events[0].labels[start_ind::]\n",
    "ml_event_times = bhv_data.segments[0].events[0].times[start_ind::]\n",
    "\n",
    "li_distance = lambda x, y: 0 if np.abs(x - y)==0 else len(neural_event_labels)\n",
    "    # manhattan_distance = lambda x, y: np.abs(x - y)\n",
    "\n",
    "_,_,_,path1 = accelerated_dtw(ml_event_labels, neural_event_labels, dist=li_distance)\n",
    "path1 = np.array(path1).T\n",
    "zero_dir = (ml_event_labels[path1[:,0]]-neural_event_labels[path1[:,1]])==0\n",
    "diff_time = ml_event_times[path1[:,0]]-neural_event_times[path1[:,1]]\n",
    "def remove_outliers(data, factor=1.5):\n",
    "    q1 = np.percentile(data, 25)\n",
    "    q3 = np.percentile(data, 75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - (factor * iqr)\n",
    "    upper_bound = q3 + (factor * iqr)\n",
    "    return data[(data > lower_bound) & (data < upper_bound)]\n",
    "diff_time = remove_outliers(diff_time, factor=1.5)\n",
    "plt.boxplot(diff_time-np.mean(diff_time))\n",
    "len(diff_time)/len(neural_event_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavior analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = bhv_data.segments[0].events[0]\n",
    "marker_24_pos = np.where(events.labels==24)[0]\n",
    "marker_5_pos = np.where(events.labels==5)[0]\n",
    "\n",
    "trial = [events[i:j+1] for i,j in zip(marker_24_pos,marker_5_pos)]\n",
    "duration = [i.times[-1]-i.times[3] for i in trial]\n",
    "trial_success = [20 in i.labels for i in trial]\n",
    "trial_time = [(i.times[-1]-trial[0].times[0]).magnitude/60 for i in trial]\n",
    "\n",
    "flag_times = bhv_data.segments[1].irregularlysampledsignals[-1].times\n",
    "training_flag = np.array(bhv_data.segments[1].irregularlysampledsignals[-1])\n",
    "trial_flag = [training_flag[np.where(j[0]<flag_times * (j[-1]>flag_times))[0]] for j in trial]\n",
    "trial_flag_times = [flag_times[np.where(j[0]<flag_times * (j[-1]>flag_times))[0]] for j in trial]\n",
    "trial_flag_times = [(i-trial[0].times[0]).magnitude.squeeze()/60 for i in trial_flag_times]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_success = []\n",
    "sliding_time = []\n",
    "mean_duration = []\n",
    "flag = []\n",
    "epoch = []\n",
    "paradigm_start = trial[0].times[0]\n",
    "\n",
    "for i in range(0,len(trial)-75):\n",
    "    mean_success.append(trial_success[i:i+75].count(True)/75)\n",
    "    mean_duration.append(np.mean(duration[i:i+75]))\n",
    "    sliding_time.append(trial_time[i+75])\n",
    "    epoch.append(i)\n",
    "    \n",
    "labels = ['Success rate']*len(mean_success) + ['Duration time']*len(mean_duration)\n",
    "value = mean_success + list(np.array(mean_duration)/max(mean_duration))\n",
    "time = sliding_time + sliding_time\n",
    "df = pd.DataFrame(zip(value, time, labels), \n",
    "                  columns = ['Percentage', 'Time (min)', 'labels'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,4))\n",
    "\n",
    "ax = sns.lineplot(data = df, x = 'Time (min)', y = 'Percentage', \n",
    "                  hue = 'labels', lw = 2.5,\n",
    "                 style = 'labels', dashes = False, markersize = 8 ,\n",
    "                 palette = ['firebrick', 'gray'])\n",
    "ax.set_xlim([df['Time (min)'][0], 38])\n",
    "ax.set_ylim([0,1.02])\n",
    "\n",
    "flag = []\n",
    "for i,j in zip(trial_flag,trial_flag_times):\n",
    "    if sum(i.squeeze()) != 0:\n",
    "        flag = flag + list(j.squeeze())\n",
    "    if sum(i.squeeze()) == 0 and len(flag) != 0:    \n",
    "        x=flag\n",
    "        y1=[0]*len(x)\n",
    "        y2=[1.02]*len(x)\n",
    "\n",
    "        ax.fill_between(x, y1, y2, #上限，下限\n",
    "                facecolor='gray', #填充颜色\n",
    "                alpha=0.3) #透明度\n",
    "        flag=[]\n",
    "\n",
    "for axis in ['bottom', 'left']:\n",
    "    ax.spines[axis].set_linewidth(2.5)\n",
    "    ax.spines[axis].set_color('0.2')\n",
    "    \n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ax.tick_params(width = 2.5, color = '0.2')\n",
    "\n",
    "plt.xticks(size = 14, weight = 'bold', color = '0.2')\n",
    "plt.yticks(size = 14, weight = 'bold', color = '0.2')\n",
    "\n",
    "ax.set_xlabel(ax.get_xlabel(), fontsize = 18, weight = 'bold', color = '0.2')\n",
    "ax.set_ylabel(ax.get_ylabel(), fontsize = 18, weight = 'bold', color = '0.2')\n",
    "\n",
    "plt.legend(frameon = False, prop = {'weight':'bold', 'size':14}, \n",
    "           labelcolor = '0.2',\n",
    "           loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial_pos\n",
    "succ = []\n",
    "\n",
    "for i,j,k in zip(trial_pos, trial_coeff, trial):\n",
    "    \n",
    "    if (i.index[0]-trial_pos[0].index[0])/60 > 30:\n",
    "        continue\n",
    "        \n",
    "    if not sum(j['training flag'].tolist())==0:\n",
    "        continue\n",
    "        \n",
    "    succ.append(0)\n",
    "    if not 20 in k.labels:\n",
    "        continue\n",
    "    succ[-1]=1\n",
    "    traj = np.array(i['pos'].tolist())\n",
    "    plt.plot(traj[:,0], traj[:,1])\n",
    "\n",
    "print('success rate: {}, trial num: {}, sucess trial num: {} (in 30 min)'.format(sum(succ)/len(succ),len(succ), sum(succ)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_time_mean = np.mean(diff_time)\n",
    "st = [i for i in neural_data.segments if i.name=='kilosort2.5'][0].spiketrains\n",
    "st_shift = [i.time_shift(diff_time_mean) for i in st if len(i.times)>3000]\n",
    "# len(st_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = bhv_data.segments[0].events[0]\n",
    "marker_24_pos = np.where(events.labels==3)[0]\n",
    "marker_5_pos = np.where(events.labels==5)[0]\n",
    "trial_event = [events[i:j+1] for i,j in zip(marker_24_pos,marker_5_pos)]\n",
    "\n",
    "marker_24_time = events.times[np.where(events.labels==24)[0]]\n",
    "marker_5_time = events.times[np.where(events.labels==5)[0]]\n",
    "trial_pos = [pos_pd.iloc[np.where(pos_pd.index<j * (pos_pd.index>i))[0]] for i,j in zip(marker_24_time,marker_5_time)]\n",
    "trial_coeff = [coeff_pd.iloc[np.where(coeff_pd.index<j * (coeff_pd.index>i))[0]] for i,j in zip(marker_24_time,marker_5_time)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elephant.kernels import GaussianKernel\n",
    "#%% data slicing\n",
    "kwargs_list = []\n",
    "trial_ind = []\n",
    "move_direction = []\n",
    "pos_input = []\n",
    "is_kwargs = {\n",
    "    'kernel' : GaussianKernel(20*pq.ms),\n",
    "    'sampling_period' : 20*pq.ms\n",
    "}\n",
    "\n",
    "for ind, i in enumerate(trial_pos):\n",
    "    if not sum(trial_coeff[ind]['training flag'].tolist())==0:\n",
    "        continue\n",
    "    if not 20 in trial[ind].labels:\n",
    "        continue\n",
    "    pos_time = np.array(i.index)*pos_unit\n",
    "    kwargs = {'t_start': pos_time[0]-5*is_kwargs['sampling_period'],\n",
    "              't_stop': pos_time[-1]+5*is_kwargs['sampling_period'],\n",
    "              'aligned_marker':[3],\n",
    "              'aligned_marker2':[5],\n",
    "              'trial_index': ind}\n",
    "    pos_input.append(i)\n",
    "    kwargs_list.append(kwargs)\n",
    "    trial_ind.append(ind)\n",
    "    move_direction.append(np.array(i['pos'].to_list()).squeeze()[-1])\n",
    "\n",
    "sliced_is = SpikeStatistics.preprocessing('instantaneous_rate', \n",
    "                                          kwargs_list, st_shift, **is_kwargs)[np.array(trial_ind)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_num = [(i['t_stop'] - i['t_start']).rescale(is_kwargs['sampling_period'].units)/is_kwargs['sampling_period'] for i in kwargs_list]\n",
    "select_is = [sliced_is[ind,:,0:int(i.magnitude)] for ind, i in enumerate(bin_num)]\n",
    "select_is = [i[:,5:-5] for i in select_is]\n",
    "fr_sum = [np.sum(i.magnitude) for i in select_is]\n",
    "select_ind = max(np.where(np.array(fr_sum)==0)[0])+1\n",
    "\n",
    "select_is = select_is[select_ind::]\n",
    "pos_input = pos_input[select_ind::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = []\n",
    "speed = []\n",
    "from scipy import signal\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "for i,j in zip(pos_input,select_is):\n",
    "    traj = np.array(i['pos'].tolist())  \n",
    "    vel = np.array(i['vel'].tolist()) \n",
    "    traj_x = gaussian_filter(signal.resample(traj[:,0],j.shape[1]+1)[1::],sigma=1)\n",
    "    traj_y = gaussian_filter(signal.resample(traj[:,1],j.shape[1]+1)[1::],sigma=1)\n",
    "    vel_x = gaussian_filter(signal.resample(vel[:,0],j.shape[1]+1)[1::],sigma=1)\n",
    "    vel_y = gaussian_filter(signal.resample(vel[:,1],j.shape[1]+1)[1::],sigma=1)\n",
    "    pos.append(np.array([traj_x,traj_y]).T)\n",
    "    speed.append(np.array([vel_x,vel_y]).T)\n",
    "    plt.plot(traj_x, traj_y)\n",
    "X_pos = np.concatenate(pos,axis=0)\n",
    "X_speed = np.concatenate(speed,axis=0)\n",
    "Y_neu = np.concatenate(select_is,axis=1).T\n",
    "\n",
    "mat_data = {}\n",
    "mat_data['neu_data'] = [i.magnitude for i in select_is]\n",
    "mat_data['pos'] = pos\n",
    "mat_data['speed'] = speed\n",
    "\n",
    "import hdf5storage\n",
    "hdf5storage.savemat(os.path.join(description_path,'dataset_not_sort.mat'), mat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import MultiTaskLasso\n",
    "from sklearn.model_selection import cross_val_score\n",
    "lasso_speed = MultiTaskLasso(fit_intercept=False)\n",
    "scores = cross_val_score(lasso_speed, Y_neu, X_speed, scoring='r2',cv=5,n_jobs=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_speed.fit(Y_neu, X_speed)\n",
    "hist, bin_edges = np.histogram(lasso_speed.coef_, bins=50)\n",
    "# plt.plot(lasso_speed.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "# data = np.random.normal(size=1000)\n",
    "\n",
    "sns.distplot(lasso_speed.coef_, hist=True, kde=False, rug=False)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
''')

def check_file(session):
    
    paradigm = ''
    paradigm = 'brain_control' if 'brain_control' in session.lower() else paradigm
    paradigm = 'data_recording' if 'data_recording' in session.lower() else paradigm
    
    quality_control = code_dict[paradigm].substitute(session0=session)
    
    dpath = os.path.join(session, 'description')
    if not os.path.exists(dpath):
        os.mkdir(dpath)
    
    with open(os.path.join(dpath, 'quality_control.ipynb'), 'w') as file:
        file.write(quality_control)

# jupyter nbconvert --execute --to notebook --inplace --allow-errors quality_control.ipynb
